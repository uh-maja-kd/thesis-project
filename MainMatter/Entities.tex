%===================================================================================
% Chapter: Propuesta para la Extracción de Entidades
%===================================================================================
\chapter{Propuesta para la Extracción de Entidades}\label{chapter:entities}
\addcontentsline{toc}{chapter}{Extracción de Entidades}
%===================================================================================

%===================================================================================

\section{Modelo}

El modelo propuesto es un \emph{StackBiLSTM-CRF} que tiene como entrada \emph{bert embeddings} pre-entrenados, \emph{postag embeddings} que son entrenados junto al modelo y \emph{char embeddings} que se entrenan junto al modelo a partir de una \textbf{CNN}. El modelo tiene como decodificadior para la predicci\'on de las etiquetas correspondientes a cada token un \textbf{CRF}.

\subsection{Entrada del Modelo}\label{sec:entrance}
Se recibe como entrada una oraci\'on en texto plano, la cual necesita preprocesamiento para construir la entrada apropiada del modelo. El primer paso es tokenizar las oraciones dado que todas las entradas del modelo esperan una secuencia de tokens.

Por cada token en el cual una oraci\'on fue dividida, la entrada respectiva a ese token consiste de una lista de tres vectores de rasgos.

\begin{description}
	\item[Vector de PoS-tag:] Es un vector \emph{one-hot} de codificaci\'on de la informaci\'on de \emph{Part of Speech} (\textbf{PoS}).
	\item[Codificaci\'on de los caracteres:] Es la concatenaci\'on de los vectores \emph{one-hot} de codificaci\'on de cada uno de los caracteres contenidos en la palabra. 
	\item[\'Bert embedding:] Es un vector de \emph{embedding} de la palabra conformado por la concatenaci\'on de los vectores que representan a dicha palabra en cada una de las capas de \textbf{BERT}.
	 
\end{description} 

Para obtener la \emph{Codificaci\'on de los caracteres} se utiliz\'o el alfabeto (VER COMO DESCRIBIMOS EL ALFABETO). Para extraer la informaci\'on de \textbf{PoS-tag} se utiliza la librer\'ia de python \textbf{spacy}~\footnote{spacy.io}. 

Para la construcci\'on de los \emph{bert embeddings} correspondientes a cada token se sigue el siguiente procedimiento. Se toma la oraci\'on y se le agregan al inicio y al final las cadenas de texto "\emph{[CLS]}" y "\emph{[SEP]}" respectivamente. Luego esta nueva oraci\'on es tokenizada por el \emph{BertTokenizer} (PONER REFERENCIA o una breve explicaci\'on de este tokenizer). Luego estos tokens son llevados a \'indices con respecto al vocabulario de \textbf{BERT}. Adem\'as se contruye la m\'ascara de atenci\'on poniendo a todos los tokens el valor de 1, dado que no es necesario para nuestros objetivos hacer \emph{padding} de las oraciones. Luego la secuencia de \'indices de los tokens en el vocabulario de \textbf{BERT} y la m\'ascara de atenci\'on son pasadas como entrada del modelo de \textbf{BERT}, obteniendo asi como salida la codificaci\'on de cada una de las capas de \textbf{BERT}.

Luego el \emph{bert embedding} de cada token se construye a partir de la concatenaci\'on de la codificaci\'on de dicho token en las 12 capas de \textbf{BERT}. Debido a la tokenizaci\'on del tipo \emph{Word Piece} realizada por el \emph{BertTokenizer} para las palabras que no se encuentran contenidas en el vocabulario de \textbf{BERT}, se crea un solo vector de \emph{embedding} resultante de calcular la media de cada uno de los vectores de \emph{embedding} correspondientes a las piezas en que se separo ese token durante la tokenizaci\'on.

\subsection{Arquitectura del Modelo}

El modelo recibe una secuencia de tokens como entrada como fue descrito en~\ref{sec:entrance}. El modelo se divide en 3 componentes: 

\begin{enumerate}
	\item Char Embedding CNN.
	\item Las BiLSTM apiladas a nivel de tokens.
	\item EL clasificador CRF. 
\end{enumerate}


El modelo est\'a organizado de la siguiente forma. Por cada token en la secuencia de entrada se construye primeramente los \emph{bert embeddings} siguiendo el procedimiento descrito en~\ref{sec:entrance}. Luego la capa \textbf{CNN} a nivel de caracteres recibe la secuencia de la codificaci\'on de cada uno de los caracteres contenidos en la palabra y produce un vector, capturando as\'i informaci\'on a nivel de caracteres en cada palabra. Estos dos vectores son concatenados junto al vector de informaci\'on de \textbf{PoS-Tag}, y todos en conjunto sirven como entrada de la primera capa \emph{BiLSTM} a nivel de palabras. La salida de esta capa correspondiente a cada token pasa por la entrada correspondiente a ese token en una segunda \emph{BiLSTM}. Finalmente la salida de esta \'ultima \emph{BiLSTM} sirve de entrada de la capa de \emph{CRF} la cual tiene como salida la etiqueta predicha para cada uno de los tokens pertenecientes a la oraci\'on. 

\subsection{Postprocesamiento}
(PONER EL POSTPROCESAMIENTO RALIZADO A LA SALIDA DEL MODELO)
La capa de CRF produce una clasificaci\'on acorde al sistema \emph{BMEWO-V} de etiquetado. Dicho sistema clasifica cada \emph{token} en \emph{B} para inicio de palabra clave, \emph{M} para continuaci\'on de palabra clave, \emph{E} para fin de palabra clave, \emph{W} para las palabras clave formadas por un solo \emph{token} y \emph{O} para los \emph{token} que no representan nada. Adem\'as, contempla la posibilidad de solapamiento entre palabras clave mediante la etiqueta \emph{V}. 
\newline
\newline
Este sistema es adaptado al escenario del \emph{IberLEF 2019}, donde se clasifican las palabras clave en \emph{Concepto}, \emph{Acci\'on}, \emph{Referencia} o \emph{Predicado}. Para ello, cada etiqueta \emph{B}, \emph{M}, \emph{E}, \emph{W} se sufija con \emph{Concepto}, \emph{Acci\'on}, \emph{Referencia} o \emph{Predicado} de acuerdo al tipo de palabra clave (la etiqueta \emph{V} puede o no tenerlo). La salida del modelo es una secuencia de estas etiquetas sufijadas. 
\newline
\newline
Para convertir la secuencia de etiquetas sufijadas que se obtiene de la oraci\'on dada, se realiza un procedimiento iterativo por la misma. Dicho procedimiento asume que el nivel de solapamiento entre palabras clave es menor o igual que dos. Esta suposici\'on, dada por las limitaciones del sistema \emph{BMEWO-V}, es lo que provoca mayor error de recobrado en este procedimiento. Dado esto, se mantienen durante la iteraci\'on los \emph{spans} de texto de las dos palabras clave que en potencia se est\'an construyendo. Estos dos objetos son actualizados en cada iteraci\'on de acuerdo a la etiqueta actual y la anterior. La etiqueta \emph{B} indica el inicio de una palabra clave, la \emph{M} la extensi\'on de la palabra clave que ya existe y la \emph{E} su finalizaci\'on. La etiqueta \emph{V} introduce un solapamiento, luego esta es la etiqueta que puede provocar que en alg\'un momento existan dos palabras clave en construcci\'on. La etiqueta \emph{W} causa que se reporte autom\'aticamente la existencia de una palabra clave en el \emph{span} de texto asociado a la misma. Luego de identificada una palabra clave, para clasificarla se utiliza un sitema de votaci\'on. Cada etiqueta que haya formado parte de una palabra clave que se report\'o como tal, "vot\'o" para decidir si era \emph{Concepto}, \emph{Acci\'on}, \emph{Referencia} o \emph{Predicado}, en correspodencia con su sufijo asociado. Cuando se reporta la palabra clave se clasifica de acuerdo a la mayor cantidad de votos que haya obtenido. Si est\'a equilibrada la votaci\'on se asume \emph{Concepto}.



















