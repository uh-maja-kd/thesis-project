%===================================================================================
% Chapter: Propuesta para la Extracción de Relaciones
%===================================================================================
\chapter{Propuesta para la Extracción de Relaciones}\label{chapter:relations}
\addcontentsline{toc}{chapter}{Extracción de Relaciones}

En este capítulo se hace una descripción de las técnicas empleadas para la resolución de la tarea de Extracción de Relaciones.
Primeramente, se recoge un resumen del análisis de dependencias de una oración, aspecto cardinal en la propuesta realizada.
Luego se describe el modelo de aprendizaje profundo propuesto, sus componentes y sus distintas variantes.


\section{Análisis de Dependencias}\label{sec:parsing}

El conocimiento de la estructura y la sintáxis que subyace en un texto en lenguaje natural puede ser de mucha ayuda en tareas típicas de NLP como la clasificación de texto, la sumarización o la extracción de relaciones.
Una de las técnicas más comunes para capturar cierta estructura en las oraciones es el análisis sintáctico~(conocido comúnmente por su nombre en inglés: \emph{parsing}).
En esta sección se abordará este tema, particularmente el análisis de dependencias.

Hay dos formas de describir la estructura de una oración en lenguaje natural: separando la oración en \textbf{constituyentes}~(frases), que se separan a su vez en constituyentes más pequeños; o estableciendo conexiones entre las palabras individuales~\cite{covington2001fundamental}.
El significado de estas dos variantes se ilustra en las figuras \ref{fig:dep_const} y \ref{fig:dep_links} respectivamente.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{Graphics/dep_const.png}
	\caption{Estructura constituyente para una oración en idioma inglés del \emph{Penn Treebank}.}\label{fig:dep_const}
\end{figure}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\linewidth]{Graphics/dep_links.png}
	\caption{Estructura de dependencias para una oración en idioma inglés del \emph{Penn Treebank}.}\label{fig:dep_links}
\end{figure}

La representación constituyente del lenguaje data de varios años atrás, y ha sido explotada tanto por los científicos de la computación como por lingüistas, en aras de obtener buenas representaciones del lenguaje natural.
Sin embargo, la comunidad científica ha mostrado un creciente interés en los últimos años en las estructuras de dependencias como una alternativa a esta representación.

La noción fundamental de \textbf{dependencia} está basada en la idea de que la estructura sintáctica de una oración está conformada por un conjunto de relaciones binarias asimétricas entre las palabras de dicha oración~\cite{nivre2005dependency}.
Siempre que se establece una relación entre dos palabras, a una de ellas se le denomina \textbf{cabecera} y a la otra \textbf{dependiente}.
A continuación se listan algunos criterios que han sido propuestos para identificar una relación sintáctica entre una cabecera $H$ y un dependiente $D$, en una construcción sintáctica $C$~\cite{zwicky1985heads, richard1990english}:

\begin{enumerate}
	\item $H$ determina la categoría sintáctica de $C$, y muchas veces puede sustituir a $C$.
	
	\item $H$ determina la categoría semántica de $C$, mientras que $D$ aporta especificidad.
	
	\item $H$ es obligatoria, mientra que $D$ es opcional.
	
	\item $H$ selecciona a $D$, y determina si puede o no ser opcional.
	
	\item La forma de $D$ depende de $H$.
	
	\item La posición de $D$ en la oración se especifica con respecto a $H$.
\end{enumerate}

Estas reglas no son absolutas y contienen un mezcla de criterios variados, algunos sintácticos, otros semánticos.
No existe en la literatura una noción coherente de dependencia que se corresponda con todos los distintos criterios~\cite{nivre2005dependency}.


\subsection{Grafo de dependencias}

Si se considera cada dependencia como un arco dirigido que tiene como origen a la cabecera y como destino al dependiente, la estructura de dependecias de la oración conforma un grafo dirigido $G$ cuyos nodos son los elementos léxicos del lenguaje~(\emph{tokens}).
Además, el grafo subyacente de $G$ debe estar conectado, para que cada nodo esté relacionado con, al menos, otro nodo.

A esta caracterización se le imponen usualmente más restricciones.
Dos de las más utilizadas en las distintas formalizaciones de gramáticas basadas en dependencias~(o simplemente, gramáticas de dependencias), son: la suposición de que cada nodo del grafo tiene \emph{indegree} $\leq 1$; y la no existencia de ciclos.
Estas suposiciones, junto a la consideración de conectividad, implican que este grafo sea un árbol dirigido con una sola raíz la cual no depende de ninguna otra palabra.
Esto último queda ilustrado en la figura \ref{fig:dep_tree}.
A esta estructura se le denomina \textbf{árbol de dependencias}.

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.7\linewidth]{Graphics/dep_tree.png}
	\caption{Estructura de dependencias para una oración en idioma inglés y árbol de dependencias.}\label{fig:dep_tree}
\end{figure}

Existen varias restricciones adicionales que se definen sobre estas estructuras y que son más debatidas.
Una de las más conocidas es la restricción de \textbf{proyectividad}~\cite{hays1964dependency,lecerf1960programme,marcus1965notion}.
Un grafo de dependencias satisface la restricción de proyectividad con respecto a un orden linear particular de los nodos si, por cada arco $h \rightarrow d$ y un nodo $w$, $w$ ocurre entre $h$ y $d$ en el orden lineal solo si $w$ está \textbf{dominado} por $h$~\footnote{La relación \textbf{dominar} es la clausura reflexiva y transitiva de la relación de dependencia definida por los arcos}. 

\subsection{Algoritmos para el análisis de dependencias}

Con basamento en estructuras de dependencias, disímiles algoritmos han sido propuestos para analizar el lenguaje natural.
De manera general pueden distinguirse dos enfoques diferentes que se han adoptado en la literatura: análisis orientado a gramáticas y análisis orientado a datos. 

Los trabajos pioneros en el análisis orientado a gramáticas se remontan a las propuestas de \textit{Hays} y \textit{Gaifman}, cuando en 1964 y 1965 respectivamente~\cite{hays1964dependency,gaifman1965dependency}, definieron un conjunto de reglas sobre las gramáticas de dependencias y un conjunto de condiciones que debían cumplir las relaciones de dependencia.
Por su parte, los primeros intentos en realizar análisis de dependencias orientado a datos por \textit{Carroll y Charniak} en 1992~\cite{carroll1992two}, pueden considerarse también orientados a gramáticas en el sentido de que apoyaron en el formalismo de una gramática de dependencias y usaron un \emph{corpus} de datos para inducir un modelo probabilístico para la desambiguación.
En esencia, usaron una Gramática Libre del Contexto Probabilística~\cite{chomsky1956three}, que estaba restringida para ser equivalente al tipo de gramáticas de \textit{Hays} y \textit{Gaifman}. 

Muchos otros trabajos se registran en la literatura relativos a propuestas de algoritmos para el análisis de dependencias~\cite{koo2008simple,mcdonald2005non,nivre2003efficient,nivre2007maltparser,socher2011parsing}.
La propuesta más común se basa en algún tipo de algoritmo de programación dinámica con o sin desambiguación estadística.


\section{Modelo}\label{sec:model}

El modelo de aprendizaje profundo propuesto se apoya en el uso de RNN sobre estructuras derivadas del árbol de dependencias de la oración de entrada y las entidades señaladas, para obtener una representación de la supuesta relación existente entre ellas.

\subsection{Hipótesis del Camino en el Árbol de Dependencias}

Como fue explicado en el capítulo~\ref{chapter:information_extraction}, la información más completa para resolver el problema de la extracción de relaciones se encuentra en la oración completa. Sin embargo, se maneja por muchos autores la suposición de que el árbol de dependencias de la oración de entrada condensa la información vital para resolver el problema, a la vez que desecha otras fuentes de desinformación.

\subsection{Red Neuronal}

Dada una oración de entrada, se utiliza un vector $w_i$ para representar la palabra $i-$'esima de la misma. Dicho vector se obtiene a partir de la concatenación de \textit{embeddings} de distintas fuentes de información:

\begin{description}
	\item[Palabras:] Se utilizan \textit{embeddings} preentrenados en un corpus construido a partir de artículos de Wikipedia con contenido médico.
	Fueron entrenados utilizado el algoritmo \textbf{word2vec}\cite{word2vec} con la arquitectura \textbf{skipgram}.
	
	\item[Caracteres:] Se utilizan \textit{embeddings} obtenidos mediante una capa BiLSTM sobre los caracteres de la palabra.
	
	\item[POST-tag y Dependencia:] Se utiliza la un \textit{embedding} de la etiqueta de POS-tag de la palabra y la dependencia de la misma con su ancestro en el árbol de dependencias de la oración.
	
	\item[Etiqueta BMEWO-V y Tipo de la entidad:] Se añaden \textit{embeddings} con información relativa a la entidad a la que pertenece la palabra en cuestión.
	En este caso la etiqueta correspondiente en el sistema BMEWO-V así como el tipo de la entidad.
	
\end{description}

Sean además, $p_1, p_2,\dots, p_k$ las posiciones de las palabras que se encuentran en el camino en el árbol de dependencias entre las dos entidades señaladas, siendo $e_1,e_2$ dichas entidades, respectivamente. Sea $W = [w_{e_1};w_{p_1};\dots;w_{p_k};w_{e_2}]$ la concatenación de los vectores en dichas posiciones. Nótese que $W$ tiene dimensión $(k+2) \times q$, siendo $q$ la cantidad de componentes de los vectores $w_i$.

Primeramente, una capa BiLSTM transforma la representación de cada palabra de la secuencia $W$, para incluir información contextual de las palabras anteriores y posteriores de cada posición:

\begin{equation*}
\overrightarrow{P} = LSTM(W)
\end{equation*}

\begin{equation*}
\overleftarrow{P} = LSTM(W)
\end{equation*}

\begin{equation*}
P = [\overrightarrow{P};\overleftarrow{P}]
\end{equation*}

Luego, la capa LSTM apilada re-enfatiza la direccionalidad de la relación procesando la secuencia $P$ solamente en el sentido que va desde el origen hacia el destino de la supuesta relación, y obtiene un vector $p$ que codifica toda la información contenida en el camino del árbol de dependencias.

\begin{equation*}
p = LSTM(P)
\end{equation*}

Entre tanto, una capa recurrente se aplica sobre el subárbol que tiene como raíz a cada una de las entidades señaladas.
En este caso se utiliza una red Tree-LSTM, en su variante $Child Sum$ \cite{treeLSTM}.
Siendo $T_{e_1}$ y $T_{e_2}$ dichos subárboles:

\begin{equation*}
	t_{e_1} = TreeLSTM(T_{e_1})
\end{equation*}


\begin{equation*}
	t_{e_2} = TreeLSTM(T_{e_2})
\end{equation*}


Una red Tree-LSTM es una generalización de una red LSTM, que permite procesar de manera recurrente estructuras de vectores que se organicen en forma de grafos dirigidos y acíclicos~(como los árboles, por ejemplo). 

Una celda Tree-LSTM en el momento $t$, al igual que su homólogo lineal, se define como una colección de vectores en $\mathbb{R}^d$, siendo $d$ la dimensión oculta de dicha celda.
Estos vectores son: una compuerta de entrada $i_t$, una compuerta de olvido $f_t$, una compuerta de salida $o_t$, una celda de memoria $c_t$ y un estado oculto $h_t$. Siendo $C(j)$ la secuencia de hijos de el nodo $j$, las ecuaciones de transición de una celda Tree-LSTM se muestran a continuación:

\begin{equation*}
	\overline{h}_j = \sum_{k\in C(j)} h_k
\end{equation*}

\begin{equation*}
i_j = \sigma(W^{(i)}x_j + U^{(i)}\overline{h}_j + b(i))
\end{equation*}

\begin{equation*}
f_{jk} = \sigma(W^{(f)}x_j + U^{(f)}h_k + b(f))
\end{equation*}

\begin{equation*}
o_j = \sigma(W^{(o)}x_j + U^{(o)}\overline{h}_j + b(o))
\end{equation*}

\begin{equation*}
u_j = tanh(W^{(u)}x_j + U^{(u)}\overline{h}_j + b(u))
\end{equation*}

\begin{equation*}
c_j = i_j \odot u_j + \sum_{k\in C(j)} f_{jk} \odot c_k
\end{equation*}

\begin{equation*}
h_j = o_j \odot tanh(c_j)
\end{equation*}

Nótese como la diferencia radica en que como se tienen varios momentos anteriores en el momento $t$, se considera en este caso la suma de sus estados ocultos.

Loa vectores obtenidos a partir de la secuencia de entrada y las entidades señaladas se concatenan para formar la representación de la hipotética relación.

\begin{equation*}
	r = [t_{e_1};t_{e_2}, p]
\end{equation*}

La salida $o$ se obtiene a partir de aplicar la función sigmoide a una transformación linear de dicho vector.

\begin{equation*}
	o = \sigma(W^{(o)}r + b^{(o)})
\end{equation*}

La matriz $W^{(o)}$ tiene dimensiones $m \times l$, donde $l = |t_{e_1}| + |t_{e_2}| + |p|$ y $m$ es el número de relaciones semánticas diferentes que se definen.

De acuerdo al vector de salida $o$, se predice la existencia de una relación si su valor máximo excede un umbral prefijado que se introduce como un hiperparámetro adicional. De ser así, se predice la existencia solamente de la relación dada por $\arg\max(o)$.
	
Nótese como, a diferencia de las arquitecturas tradicionales, esta configuración permite prescindir de la relación ficticia \textit{none}.

La figura \ref{fig:rel_model} ilustra la arquitectura descrita.

\begin{figure}[h!]
	\centering
	\includegraphics[width=1\linewidth]{Graphics/rel_model_class.jpg}
	\caption{Arquitectura de red utilizada. La oración de entrada es \textit{El cáncer de pulmón puede causar muerte prematura} Y las entidades en cuestión son \textit{cáncer de pulmón} y \textit{muerte}.}\label{fig:rel_model}
\end{figure}

%===================================================================================